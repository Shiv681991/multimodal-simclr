This is the repository for the work titled "Domain-aware Self-supervised Pre-training for Label-Efficient Meme Analysis", accepted at AACL-IJCNLP-2022.
This has been forked from https://github.com/khizirsiddiqui/multimodal-simclr.git

arXiv Link: https://arxiv.org/abs/2209.14667

![arxivshotAACL](https://user-images.githubusercontent.com/9869470/199331421-fc7697b8-75b6-4670-b131-044ba4d61ed9.png)

<!-- <strong>ABSTRACT:</strong> <br>
Existing self-supervised learning strategies are constrained to either a limited set of objectives or generic downstream tasks that predominantly target uni-modal applications. This has isolated progress for imperative multimodal applications that are diverse in terms of complexity and domain-affinity, such as meme analysis. Here, we introduce two self-supervised pre-training methods, namely Ext-PIE-Net and MM-SimCLR that (i) employ off-the-shelf multi-modal hate-speech data during pre-training and (ii) perform selfsupervised learning by incorporating multiple specialized pretext tasks, effectively catering to the required complex multi-modal representation learning for meme analysis. 
We experiment with different self-supervision strategies, including potential variants that could help learn rich cross-modality representations and evaluate using popular linear probing on the Hateful Memes task. The proposed solutions strongly compete with the fully supervised baseline via label-efficient training while distinctly outperforming them on all three tasks of the Memotion challenge with 0:18%, 23:64%, and 0:93% performance gain, respectively. Further, we demonstrate the generalizability of the proposed solutions by reporting competitive performance on the HarMeme task. Finally, we empirically establish the quality of the learned representations by analyzing task-specific learning, using fewer labeled training samples, and arguing that the complexity of the self-supervision strategy and downstream task at hand are correlated. Our efforts highlight the requirement of better multi-modal self-supervision methods involving specialized pretext tasks for efficient fine-tuning and generalizable performance. 
 -->
 
<!-- <div style="width:60px ; height:60px"> -->
![aaclmodel](https://user-images.githubusercontent.com/9869470/199330777-4695fec8-bc20-4cc1-a2a5-78bc3116ef1c.png)
<!-- <div> -->
<!-- <img src="[http://url/image.png](https://user-images.githubusercontent.com/9869470/199330777-4695fec8-bc20-4cc1-a2a5-78bc3116ef1c.png)" height="200" width="200" > -->

For downloading the pre-trained weights, refer: https://drive.google.com/file/d/1GXvMFvSvspWx0-yAybPUd_7bJ_-yVf0n/view?usp=sharing
